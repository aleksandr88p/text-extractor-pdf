# Документация по проекту поиска в Уголовном кодексе (Código Penal)

## Общая архитектура

Система состоит из следующих основных компонентов:

1. **Извлечение текста** (`pdf_extractor.py`, `best_attempt.py`) - извлечение текста из PDF файла кодекса.
2. **Обработка текста** (`clean_text.py`) - очистка и нормализация извлеченного текста.
3. **Группировка статей** (`group.py`) - объединение статей в логические группы по книгам/разделам/главам.
4. **Чанкирование** (`chunking.py`) - разбиение текста на оптимальные фрагменты для поиска.
5. **Создание эмбеддингов** (`generator.py`) - создание векторных представлений для чанков.
6. **Поиск** (`search.py`) - гибридный поиск по тексту кодекса (текстовый + векторный).
7. **Бот-помощник** (`legal_bot.py`) - формирование ответов на основе найденных фрагментов.

## Улучшения поисковой системы

### 1. Гибридный поиск (ключевое улучшение)

Реализован в `search.py`. Комбинирует два метода поиска:
- **Текстовый поиск** - прямое сопоставление ключевых слов из запроса
- **Векторный поиск** - семантический поиск с использованием FAISS индекса

Алгоритм:
1. Извлечение ключевых юридических терминов из запроса
2. Поиск прямых текстовых совпадений этих терминов в чанках
3. Параллельное выполнение векторного поиска по эмбеддингам
4. Комбинирование результатов с приоритетом текстовых совпадений
5. Маркировка типа совпадения (текстовое/векторное) для прозрачности

```python
# Пример ключевой функции гибридного поиска
def search_similar_chunks(question, chunks, index_path, model_name, top_k):
    # 1. Извлекаем ключевые термины из запроса
    key_terms = extract_key_terms(question.lower())
    
    # 2. Выполняем текстовый поиск
    text_matches = []
    for idx, chunk in enumerate(chunks):
        chunk_text = chunk["text"].lower()
        score = 0
        for term in key_terms:
            if term in chunk_text:
                score += 1
        
        if score > 0:
            text_matches.append({"idx": idx, "score": score, "chunk": chunk.copy()})
    
    # 3. Выполняем векторный поиск
    # ... (загрузка индекса, модели, создание эмбеддингов, поиск)
    
    # 4. Комбинируем результаты
    # ... (приоритизация текстовых совпадений, удаление дубликатов)
    
    return combined_results
```

### 2. Разбиение на чанки с учетом структуры документа

Реализовано в `chunking.py`. Создает логические фрагменты текста, следуя структуре кодекса:

1. Поиск границ статей с помощью регулярных выражений
2. Разделение по статьям, если они обнаружены
3. Дополнительное разбиение больших фрагментов с сохранением смысловой структуры
4. Сохранение метаданных о структуре (libro, título, capítulo, статьи)

### 3. Улучшенная индексация для векторного поиска

Реализовано в `generator.py`. Оптимизирует создание эмбеддингов:

1. Использование многоязычных моделей для лучшего понимания испанского текста
2. Пакетная обработка для ускорения
3. Выбор оптимального типа индекса в зависимости от объема данных

## Интеграция с LLM (руководство)

Для интеграции с LLM в вашем API проекте:

### 1. Передача контекста в LLM

```python
def get_legal_answer(question, search_results):
    # Подготовка контекста из результатов поиска
    context = "\n\n".join([chunk["text"] for chunk in search_results])
    
    # Формирование промпта для LLM
    prompt = f"""
    На основе следующих фрагментов Уголовного кодекса Испании (Código Penal) ответь на вопрос:
    
    Вопрос: {question}
    
    Фрагменты кодекса:
    {context}
    
    Ответь точно, опираясь только на предоставленные фрагменты. Если информации недостаточно, укажи это.
    """
    
    # Запрос к LLM API (реализуйте в соответствии с вашим API)
    answer = call_llm_api(prompt)
    
    return answer
```

### 2. Структура API эндпоинта

```python
@app.route('/legal_question', methods=['POST'])
def legal_question():
    data = request.json
    question = data.get('question')
    
    # 1. Выполняем гибридный поиск
    search_results = search_similar_chunks(
        question, 
        chunks,  # Загружайте чанки при запуске приложения
        index_path,
        model_name,
        top_k=5
    )
    
    # 2. Получаем ответ от LLM
    answer = get_legal_answer(question, search_results)
    
    return jsonify({
        'question': question,
        'answer': answer,
        'sources': search_results  # Опционально: для отображения источников
    })
```

## Оптимизация производительности

1. **Кэширование результатов**:
   - Добавьте кэширование часто задаваемых вопросов
   - Кэшируйте загруженные модели и индексы

2. **Асинхронное выполнение**:
   - Используйте асинхронные вызовы для API запросов
   - Параллельное выполнение текстового и векторного поиска

## Основные файлы для переноса в другой проект

- `search.py` - Гибридный поиск (ключевая реализация)
- `chunking.py` - Логика разбиения на чанки
- `generator.py` - Создание эмбеддингов
- `legal_bot.py` - Интеграция с LLM

## Дополнительные возможности (опционально)

1. **Улучшенное чанкирование с LLM** (`llm_chunking.py`):
   - Использование LLM для анализа и структурирования документа
   - Семантическое разбиение с пониманием юридического контекста

2. **Расширение запросов**:
   - Добавление юридических синонимов и связанных терминов
   - Переформулировка запроса с использованием LLM

## Пример полного flow для API проекта

```python
# 1. Загрузка необходимых данных при старте сервера
chunks = load_chunks('path_to_chunks.json')
index = faiss.read_index('path_to_index')
model = SentenceTransformer('model_name')

# 2. Обработка запроса
def process_legal_question(question):
    # Поиск релевантных фрагментов
    results = search_similar_chunks(question, chunks, index, model)
    
    # Формирование ответа с LLM
    answer = get_legal_answer(question, results)
    
    return answer, results
```

## Требования к зависимостям

```
sentence-transformers>=2.2.2
faiss-cpu>=1.7.0  # или faiss-gpu
numpy>=1.22.0
requests>=2.28.0
```

## Дополнительная документация

- [Документация FAISS](https://github.com/facebookresearch/faiss/wiki)
- [Sentence Transformers](https://www.sbert.net/)