"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º –£–≥–æ–ª–æ–≤–Ω–æ–≥–æ –∫–æ–¥–µ–∫—Å–∞

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É –£–≥–æ–ª–æ–≤–Ω–æ–≥–æ –∫–æ–¥–µ–∫—Å–∞
—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º FAISS –∏–Ω–¥–µ–∫—Å–∞ –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
"""

import faiss
import numpy as np
import json
import os
import re
from sentence_transformers import SentenceTransformer

def search_similar_chunks(
    question: str,
    chunks: list,
    index_path: str = "output/penal_code.index",
    model_name: str = "all-MiniLM-L6-v2",
    top_k: int = 5
) -> list:
    """
    –ò—â–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ —á–∞–Ω–∫–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏.
    
    Args:
        question (str): –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        chunks (list): –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
        index_path (str): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–Ω–¥–µ–∫—Å–∞
        model_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        top_k (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
        
    Returns:
        list: –°–ø–∏—Å–æ–∫ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö —á–∞–Ω–∫–æ–≤ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
    """
    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –∑–∞–ø—Ä–æ—Å–µ
    key_terms = extract_key_terms(question.lower())
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–∑–æ–≤–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    text_matches = []
    for idx, chunk in enumerate(chunks):
        chunk_text = chunk["text"].lower()
        score = 0
        for term in key_terms:
            if term in chunk_text:
                score += 1
        
        if score > 0:
            text_matches.append({
                "idx": idx,
                "score": score,
                "chunk": chunk.copy()
            })
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
    text_matches.sort(key=lambda x: x["score"], reverse=True)
    
    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ö–æ—Ç—è –±—ã –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É, –±–µ—Ä–µ–º –∏—Ö
    direct_results = []
    if len(text_matches) >= 2:
        direct_results = [match["chunk"] for match in text_matches[:3]]
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º FAISS index –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    index = faiss.read_index(index_path)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    model = SentenceTransformer(model_name)
    
    # –°—á–∏—Ç–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑–∞–ø—Ä–æ—Å–∞
    question_embedding = model.encode([question], convert_to_numpy=True)
    
    # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–µ K
    distances, indices = index.search(question_embedding, top_k)
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    vector_results = []
    for idx, dist in zip(indices[0], distances[0]):
        if idx < len(chunks):
            result = chunks[idx].copy()
            result["distance"] = float(dist)
            vector_results.append(result)
    
    # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    # –û—Ç–¥–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º
    combined_results = []
    seen_indices = set()
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–Ω–∞—á–∞–ª–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
    for result in direct_results:
        key = (result["libro"], result["titulo"], result["capitulo"], result["chunk_index"])
        if key not in seen_indices:
            seen_indices.add(key)
            result["match_type"] = "text_match"
            combined_results.append(result)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è, –µ—Å–ª–∏ –∏—Ö –µ—â–µ –Ω–µ—Ç –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
    for result in vector_results:
        key = (result["libro"], result["titulo"], result["capitulo"], result["chunk_index"])
        if key not in seen_indices:
            seen_indices.add(key)
            result["match_type"] = "vector_match"
            combined_results.append(result)
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    return combined_results[:top_k]

def extract_key_terms(question):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞.
    """
    # –°–ø–∏—Å–æ–∫ –±–∞–∑–æ–≤—ã—Ö —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –Ω–∞ –∏—Å–ø–∞–Ω—Å–∫–æ–º
    basic_terms = [
        "prescripci√≥n", "plazo", "delito", "homicidio", "pena", "robo", 
        "estafa", "hurto", "violencia", "asesinato", "alevos√≠a", 
        "atenuante", "agravante", "responsabilidad", "civil", "penal"
    ]
    
    # –ü–æ–∏—Å–∫ —ç—Ç–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ –≤–æ–ø—Ä–æ—Å–µ
    found_terms = []
    for term in basic_terms:
        if term in question:
            found_terms.append(term)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–∞ –∏ —Ç–µ—Ä–º–∏–Ω—ã –≤–∏–¥–∞ "art√≠culo X"
    numbers = re.findall(r'\d+', question)
    article_refs = re.findall(r'art[√≠i]culo\s+\d+', question)
    
    found_terms.extend(numbers)
    found_terms.extend(article_refs)
    
    return found_terms

def search_by_article_number(article_number: str, chunks: list) -> list:
    """
    –ò—â–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Å—Ç–∞—Ç—å—é –ø–æ –µ—ë –Ω–æ–º–µ—Ä—É.
    
    Args:
        article_number (str): –ù–æ–º–µ—Ä —Å—Ç–∞—Ç—å–∏
        chunks (list): –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
        
    Returns:
        list: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö —É–∫–∞–∑–∞–Ω–Ω—É—é —Å—Ç–∞—Ç—å—é
    """
    results = []
    
    for chunk in chunks:
        if article_number in chunk["article_numbers"]:
            results.append(chunk)
            
    return results

def format_search_results(results: list) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞.
    
    Args:
        results (list): –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ–∏—Å–∫–∞
        
    Returns:
        str: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    output = []
    
    for i, chunk in enumerate(results, 1):
        match_type = ""
        if "match_type" in chunk:
            if chunk["match_type"] == "text_match":
                match_type = " üî§ [–¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ]"
            elif chunk["match_type"] == "vector_match":
                match_type = " üîç [–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ]"
        
        output.append(f"\n--- –†–µ–∑—É–ª—å—Ç–∞—Ç {i}{match_type} {'='*40}")
        output.append(f"üìö –ö–Ω–∏–≥–∞: {chunk['libro']}")
        
        if chunk['titulo']:
            output.append(f"üìñ –†–∞–∑–¥–µ–ª: {chunk['titulo']}")
            
        if chunk['capitulo']:
            output.append(f"üìë –ì–ª–∞–≤–∞: {chunk['capitulo']}")
            
        output.append(f"üìã –°—Ç–∞—Ç—å–∏: {', '.join(chunk['article_numbers'])}")
            
        output.append(f"\n{chunk['text']}\n")
    
    return "\n".join(output)

def extract_article_number(question: str):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä —Å—Ç–∞—Ç—å–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞, –µ—Å–ª–∏ –µ—Å—Ç—å.
    
    Args:
        question (str): –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
    Returns:
        str: –ù–æ–º–µ—Ä —Å—Ç–∞—Ç—å–∏ –∏–ª–∏ None
    """
    # –ò—â–µ–º –∑–∞–ø—Ä–æ—Å—ã –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å—Ç–∞—Ç—å—è—Ö
    article_match = re.search(r'(art√≠culo|articulo|—Å—Ç–∞—Ç—å—è|—Å—Ç–∞—Ç—å–∏|art\.?)\s*(\d+)', question, re.IGNORECASE)
    if article_match:
        return article_match.group(2)
    return None

if __name__ == "__main__":
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    chunks_file = "output/penal_code_chunks.json"
    index_path = "output/penal_code.index"
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–æ–≤
    if not os.path.exists(chunks_file):
        alternative_path = "penal_code_chunks.json"
        if os.path.exists(alternative_path):
            chunks_file = alternative_path
        else:
            print(f"–û—à–∏–±–∫–∞: –§–∞–π–ª —Å —á–∞–Ω–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –≤ {chunks_file}, –Ω–∏ –≤ {alternative_path}")
            exit(1)
            
    if not os.path.exists(index_path):
        print(f"–û—à–∏–±–∫–∞: –ò–Ω–¥–µ–∫—Å–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {index_path}")
        exit(1)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —á–∞–Ω–∫–∏
    with open(chunks_file, "r", encoding="utf-8") as f:
        chunks = json.load(f)
    
    print("üîé –ü–æ–∏—Å–∫ –≤ –£–≥–æ–ª–æ–≤–Ω–æ–º –∫–æ–¥–µ–∫—Å–µ")
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞")
    
    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –ø–æ–∏—Å–∫–∞
    while True:
        question = input("\n–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å (–∏–ª–∏ 'q' –¥–ª—è –≤—ã—Ö–æ–¥–∞): ")
        if question.lower() in ['q', 'quit', 'exit']:
            break
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç—Å—è –ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Å—Ç–∞—Ç—å—è
        article_number = extract_article_number(question)
        
        if article_number:
            print(f"–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç—å–∏ {article_number}...")
            results = search_by_article_number(article_number, chunks)
            if not results:
                print(f"–°—Ç–∞—Ç—å—è {article_number} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–æ–¥–µ–∫—Å–µ.")
                # –ü—Ä–æ–±—É–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
                results = search_similar_chunks(question, chunks, index_path)
        else:
            # –û–±—ã—á–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
            results = search_similar_chunks(question, chunks, index_path)
        
        if results:
            print(format_search_results(results))
        else:
            print("–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")